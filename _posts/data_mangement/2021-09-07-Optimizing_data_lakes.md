---
title: "Optimising data lakes"
last_modified_at: 2023-09-07T12:54:02-05:00
categories:
  - Blog
tags:
  - data lake
---

#### 최적의 데이터 레이크 구축을 위한 4가지 핵심 질문
> 데이터 레이크를 사용함으로써 기업은 이전보다 잠재적으로 더 많은 것들을 할 수 있다.   
> 이전의 단절된 데이터 셋들을 결함함으로써 인사이트를 을 수 있고 업무를 최적화할 수도 있다.   
> 하지만 어떻게 아키텍처를 설계하고 어떻게 구현결과가 의미있는 영향을 미치도록 할 것인가?

[참조사이트](https://blog.scottlogic.com/acarr/assets/data-lakes-white-paper.pdf)

#### 데이터 레이크는 절대 만능이 아니다
> 많은 금융서비스 기관들은 이미 잠재적으로 데이터 레이크의 유용성에 대해서 인식하고 있다.   
> 세계적으로 기업들은 그들의 시장과 고객을 좀 더 잘 이해하기 위해서 그들의 IT 시스템을 확장하고 있다.  
> 그 결과로 데이터를 Pool하고 회사 전반적으로 나타나는 모든 것에 대한 자세한 뷰를 얻기 위한 수단으로 데이터 레이크가 출현하였다.  

특히 금융서비스 분야에 있어서 추가적인 동력이 있다.   
2008년의 금융위기 이후 10여년 지나고, 리스크에 대한 총체적인 관점을 확보하는 것의 가치가 중요하다는 것을 점점 더 인식하게 되었다.   
여러 기업에서 사용할 수 있는 모든 데이터의 보다 완전한 뷰를 가지고 이슈를 인식하는 것과 그렇지 않은 것은 다를 수 있다.   

> 표면적으로 데이터 레이크는 많은 기회를 제공하는 것처럼 보인다.
> 이것을 사용함으로써 셀프서비스 레포팅, 규제요구사항 만족, 단일소스 레퍼런스 데이터 수집과 새로운 목적의 데이터 오픈 등의 옵션을 가질 수 있다.

예를 들면, 좀 더 확립된 데이터웨어하우스는 저장된 데이터가 (특정 데이터를 매우 빨리 찾아주는 stat schema와 같은) 여러 특정 포맷으로 변환되길 요구한다.   
데이터 레이크는 모든 데이터가 원시포맷으로 저장되는 것을 허용한다.   
일단 데이터의 혁신적 사용 또는 값어치를 발견한다면 사용자들은 나중에 그것을 자유롭게 변환할 수 있다.   
이러한 용도 중 일부는 원시 정보를 훌륭하고 세부적으로 분석할 때만 나타납니다.   

데이터 레이크는 또한 다양한 소스의 전체 데이터에 대한 유용한 저장소를 제공하므로 보다 정확한 정보를 제공하는 데 사용할 수 있습니다.
위험 보고 및 통찰력. 기업은 이전에 다양한 사일로에 보관된 데이터를 결합함으로써 고객 상호 작용 및 운영에 대해 보다 통합된 보기를 얻을 수 있습니다. Hadoop과 같은 프레임워크의 출현으로 상용 하드웨어에 막대한 양의 데이터를 저장하는 데 드는 비용은 대규모 데이터 저장소 보유의 이점을 알고 있는 기업의 거래를 더욱 유리하게 만들었습니다.

그러나 모든 현명한 기업은 만병통치약 같은 것이 없다는 것을 알고 있습니다. 많은 금융 서비스 회사가 데이터 레이크의 잠재적인 기회를 활용하려고 시도하는 동안 실망스러운 것부터 재앙적인 것까지 다양한 문제에 직면합니다.

언제나 그렇듯이 성공을 보장하는 것은 기술이 아니라 기업이 이를 활용하기로 결정하는 것입니다. 따라서 데이터 레이크가 기대에 미치지 못하는 경우 근본 원인을 파악하고 성능을 최적화하기 위해 스스로에게 물어볼 수 있는 몇 가지 질문은 다음과 같습니다   

#### 데이터 레이크란 무엇인가?  
> 데이터 레이크는 유연성으로 유명합니다. 본질적으로 데이터 레이크는 다양한 소스의 원시 데이터를 보관하고 최선의 용도를 결정할 때까지 그대로 둘 수 있는 장소입니다.

데이터 웨어하우스는 "ETL(Extract Transform Load)" 접근 방식을 사용하여 채워집니다. 즉, 정보가 저장되기 전에 특정 용도에 맞게 조정됩니다. 이와 대조적으로 데이터는 일반적으로 "ELT(Extract Load Transform)" 패턴에 따라 변환되기 전에 데이터 레이크에 로드됩니다.

ELT에는 장점이 있을 수 있습니다. 예를 들어, 규제 기관이 잠재적인 시장 조작 사건을 조사하기 위해 특정 주식에 대해 어떤 거래가 수행되었는지 문의한다고 가정해 보겠습니다. 해당 거래가 주문된 방식을 이해하기 위해 각 거래자가 시스템에 입력한 내용에 대한 정보가 필요할 수 있습니다.

여러 시스템을 검사할 수 있다는 것은 회사가 문의부터 전환, 전화 통화 연결, 인스턴트 메시지 및 웹 트래픽까지 고객의 전체 여정을 볼 수 있다는 것을 의미합니다. 정확한 타임스탬프와 결합된 이 정보는 판매 전환 프로세스를 개선하는 데 매우 중요할 수 있으며 원시 데이터를 사용하여 훨씬 쉽게 검사할 수 있습니다.

이는 전체 원시 데이터를 검토해야만 적절하게 해결할 수 있는 질문입니다. 데이터 웨어하우스나 원래 거래 시스템의 최적화된 스토리지와 같은 온라인 거래 처리 시스템(OLTP)에 저장된 최적화된 데이터에는 특정 정보가 포함되지 않을 수 있습니다.

> 필요한 모든 데이터가 향후 모든 용도에 맞게 저장되도록 하는 유일한 방법은 로그를 포함하여 데이터를 원시 상태로 유지하거나 데이터 손실 없이 데이터를 변환하는 것입니다.

#### Q1: 데이터를 변환할 것인가? RAW 상태로 저장할 것인가?
> 데이터 레이크의 잘 알려진 이점은 향후 다양한 용도로 원시 데이터를 저장할 수 있다는 것입니다. 하지만 그것이 정말로 귀하의 비즈니스에 필요한 것입니까?

이론적으로 데이터 레이크의 강력한 용도는 이를 거대한 장난감 상자로 사용하여 향후 연구를 위해 방대한 양의 다양한 데이터를 한 곳에 보관하는 것입니다. 그러나 실제로 그렇게 해야 하는지 여부는 데이터 레이크를 어떤 용도로 사용하려는지에 따라 다릅니다.

일부 회사는 아마도 새로운 시스템의 최적화되지 않은 프로토타입을 만들기 위한 목적으로 데이터에 대한 탐색 작업을 수행하는 경우 원시 데이터가 포함된 데이터 레이크를 구축하기를 원할 수 있습니다. 그러나 데이터 레이크에 대해 정의된 목적이 있는 경우 데이터를 변환하지 않으면 종종 비생산적일 수 있습니다.   

첫째, 데이터가 해당 용도에 맞게 최적화되지 않으며 데이터 레이크에 충돌하거나 중복되거나 부정확한 데이터가 포함될 수 있습니다.   

둘째, 데이터를 변환하지 않으면 소스 시스템의 특성이 그대로 유지됩니다. 여러 소스에서 데이터를 그리는 경우 문제가 더욱 악화될 수 있습니다.   

셋째, 데이터를 검색할 때 여전히 변환해야 하므로 개발 프로세스가 더욱 느려집니다. 여러 개발자가 이를 변환하기 위해 서로 다른 함수를 작성하는 경우 동일한 소스 데이터에서 일관성 없는 결과가 생성될 위험도 있습니다.   

데이터를 저장하기 전에 변환하는 것은 데이터 레이크의 주요 이점 중 하나에 반하는 것처럼 보입니다. 그러나 그렇게 하면 오해의 소지가 있거나 잘못된 데이터 분석으로 인해 발생하는 비즈니스 문제를 예방할 수 있습니다.   

회사에 여러 CRM(고객 관계 관리) 시스템이 있고 클라이언트가 상호 작용할 수 있는 다양한 방법이 있는 일반적인 예를 들어 보겠습니다. 회사는 이러한 여러 CRM 시스템의 모든 고객 상호 작용을 저장한 다음 이를 분석하여 교차 판매 기회를 파악할 수 있었습니다. 이러한 경우 데이터를 올바른 상태로 만드는 데 필요한 일반적인 변환이 많이 있습니다.   

값이 일관되고 특정 값이 데이터 레이크 전체에서 동일한 의미를 갖습니까? 중복된 데이터로 인해 별도 시스템의 데이터가 연결되지 않습니까? 각 시스템에서 클라이언트 이름이 일관되게 지정되어 있습니까?   

이러한 변환이 얼마나 많은 작업을 수행할 수 있는지 과소평가하기 쉽습니다. 실제로 팀이 여러 시스템의 소스 데이터를 사용할 때 이는 프로젝트에서 가장 시간이 많이 걸리는 작업이 될 수 있습니다.   

이러한 경우 풍부한 변환 기능을 갖춘 데이터 레이크용 ETL 도구 채택을 고려할 수 있습니다. 이러한 도구는 상당한 시간과 노력을 절약할 수 있으며, 많은 도구가 데이터 레이크로 데이터를 가져오는 방향으로 최적화되도록 특별히 발전하고 있습니다.   

최적화된 데이터를 사용하고 있지만 여전히 원시 데이터를 유지해야 하는 강력한 동인이 있는 경우 세 번째 옵션을 고려해 볼 가치가 있습니다. 여기에는 원시 데이터를 분리하고 최적화된 데이터를 다른 영역으로 변환하는 작업이 포함됩니다. 이와 같은 접근 방식을 통해 분석가와 애플리케이션은 각자의 목적에 맞게 각 데이터 세트를 활용할 수 있습니다. 그러나 두 배의 데이터를 저장해야 하며 두 데이터 세트를 동일한 데이터 레이크 내에 유지해야 하는 복잡성이 추가됩니다.   

> 데이터를 언제 변환해야 하는지 아는 것은 까다롭지만 매우 중요합니다.
> 너무 일찍 변환하면 원시 데이터에 포함된 미묘한 정보가 손실될 위험이 있습니다.
> 변환이 너무 늦어지면 소비 프로세스나 형식을 정확하게 이해하지 못하는 데이터 과학자에게 책임이 부여됩니다.
> 무엇을 할지 결정하는 것은 주로 데이터 레이크로 무엇을 하려는지에 따라 달라집니다.
> 그러나 "원시" 영역과 "정제된" 영역 모두에 데이터를 저장하는 세 번째 옵션을 고려할 수 있습니다.
> 이는 확실한 솔루션이 될 수 있지만 두 접근 방식의 장점과 단점을 모두 갖고 있으므로 주의해서 다루어야 합니다.


#### Q2: 수집하는 데이터를 어느 정도로 필터링 할 것인가?   
> 다양한 소스의 데이터가 같은 장소에 저장되면 품질이 낮은 데이터가 전체 풀을 오염시킬 수 있습니다.


많은 시스템의 콘텐츠를 데이터 레이크에 버릴 때 어떤 조직이라도 보편적으로 높은 데이터 품질을 보장하기는 어렵습니다.

결국 데이터는 요구 사항의 많은 변화, 맞춤형 시스템의 유기적 성장, 중복이 많은 IT 환경의 영향을 받게 됩니다.

데이터 레이크에 여러 데이터 세트를 저장하는 데 따른 한 가지 부작용은 데이터의 일부가 "죽은 상태에서 다시 가져온다"는 것입니다. 이들 중 일부는 손상되었거나 오래된 데이터 열일 수 있습니다. 다른 것들은 더 이상 운영 용도로 활용되지 않는 시스템 구석에 위치한 데이터일 수 있습니다.   


> 데이터 레이크에는 도움이 되지 않는 데이터가 표면으로 올라와 전체 풀을 오염시키는 것을 막기 위한 제한이 없을 수도 있습니다.

데이터 레이크가 셀프 서비스 보고와 같은 운영 프로세스를 제공하는 데 사용되거나 클라이언트에게 공개되는 경우, 그렇지 않으면 시스템이 운영 용도로 부적절한 데이터를 끌어낼 위험이 있다는 점을 인식해야 합니다. 먼저 정보를 선별해 보세요.

다시 한번 말씀드리지만, 데이터 레이크를 통해 사용자가 데이터를 선별하지 않고 데이터를 보관할 수 있다고 해서 이것이 반드시 최선의 선택이라는 의미는 아닙니다. 데이터 레이크가 데이터 품질 및 중복 문제에 직면한 경우 항상 더 가까이 다가가서 잘못된 데이터를 필터링하거나 버리고, 처음에 어떤 데이터 소스와 어떤 테이블과 열이 데이터 레이크로 유입되는지 추가로 감독하는 것이 좋습니다.

기능이 풍부한 ETL 도구의 출현으로 이제 거의 모든 사람이 데이터 레이크에 데이터를 배치할 수 있으므로 이를 수행할 수 있는 사람을 제한하기 위해 제어 장치를 마련해야 하는지 고려하는 것이 유용합니다.

이전 지점에서는 다양한 유형의 데이터를 포함하는 "영역"의 사용에 대해 논의했습니다. 변환된 데이터에 대한 "세련된 영역" 설정에 관심이 있는 경우 여기에는 "고품질 데이터"만 포함되어야 한다는 점을 기억하세요. 따라서 올바른 형식과 품질을 갖춘 데이터만 이 영역으로 전달될 수 있는지 확인해야 합니다.   

데이터 레이크는 수집하는 데이터의 잠재력과 한계를 완전히 인식하지 못하는 조직에서 오용되는 경우가 많습니다. 데이터의 품질과 성격에 대한 통찰력을 가지면 데이터 레이크의 용도가 무엇인지 결정됩니다.

> 모든 데이터를 데이터 레이크에 버리고 나중에 사용할 가치가 있는 것이 무엇인지 알아내야 합니까? 품질이 확인된 데이터만 전송해야 합니까?
> 언제나 그렇듯, 어떤 접근 방식을 취하는지는 귀하의 요구사항에 따라 다릅니다.
> 데이터 레이크를 데이터 과학 발견 프로젝트를 위한 채굴 장소로 생각한다면 전자를 선택하는 것이 좋습니다.
> 소프트웨어 팀이 해당 데이터를 활용하여 애플리케이션을 구축하도록 하려면 후자를 선택할 수 있습니다. 이전 지점에서 언급했듯이 데이터를 원시 형식과 변환된 형식으로 저장하여 데이터를 "두 배로 늘리면" 두 접근 방식의 단점이 모두 제거되지는 않지만 두 가지 장점을 모두 얻을 수 있습니다.

#### Q3: 배치형태로 수집할 것인가? 스트리밍 형태로 수집할 것인가?   
> 많은 금융 기관에서는 데이터를 일괄 처리합니다. 하지만 스트리밍이 더 가치 있는 솔루션을 제공할 수 있을까요?

금융 서비스 회사가 일괄 처리를 사용하여 데이터를 전송하는 것은 흔한 일입니다. 이는 금융 산업의 일상적인 접근 방식을 모방한다는 점에서 의미가 있습니다. 거래소의 거래 창구 사이에서 정보는 팀의 파이프라인을 따라 전달되며, 티켓이 수집되어 전달되는 기존 방식을 거의 모방합니다.
상인. 그러나 기관에서 대신 스트리밍을 고려하는 사례가 강력하고 늘어나고 있습니다.  

스트리밍 모델이 구현되면 운영 비용을 낮출 수 있지만 기업은 스트리밍 아키텍처를 개발하는 데 드는 초기 비용을 부담해야 합니다. 이를 수행하려는 기업의 의지는 전송하는 데이터의 용도와 공급되는 데이터 레이크의 목적에 전적으로 달려 있습니다.  

데이터 레이크를 시간에 민감하지 않은 저장소로 사용하는 회사에서는 일괄 처리 방식이 적합할 수 있습니다. 사용이 더 중요하거나 실시간(또는 적어도 상대적으로 최신) 정보에 의존하는 경우 스트리밍이 더 나은 옵션으로 나타나는 경우가 많습니다.  

회사는 거래일이 끝나는 등 거래가 중단되는 동안 일괄 작업을 시작할 수 있습니다. 일괄 작업에 한 시간이 걸리고 실패하는 경우 일반적으로 작업이 완료될 때까지 전체 작업을 다시 실행해야 합니다. 처리되는 데이터의 양이 증가함에 따라 일괄 처리를 완료하는 데 시간이 더 오래 걸립니다.  

게다가 거래는 글로벌하고 빠르게 진행됩니다. 일부 배치 작업은 특정 시간 내에 완료해야 하며, 그렇지 않으면 거래가 재개될 때 특정 제품이나 시장에 대한 거래가 차단되는 등 영향을 미칠 수 있습니다. 시스템이 다른 작업을 위해 "작동"되어야 하는 동안 일괄 작업을 실행하는 문제도 있습니다.  

데이터 레이크는 관련 원본 데이터와 함께 OLTP(온라인 트랜잭션 처리) 시스템에 의해 공급될 수 있지만 전송 프로세스가 시간 제한이 있는 일괄 작업인 경우 최신 정보가 없을 수 있습니다. 이는 특히 고객에게 거래 정보를 제공하거나 조직에 데이터의 "단일 버전"을 제공하는 데 데이터 레이크가 사용되는 경우 결과를 초래할 수 있습니다.  

데이터 레이크를 중앙 집중식 참조 데이터 소스 또는 "단일 버전의 진실"로 사용하는 데에는 고유한 문제가 발생할 수 있습니다. 일반적으로 중요하지 않은 시스템인 데이터 레이크를 기본적으로 가져와 일부 중요 시스템에 종속되게 만드는 것입니다. 이는 엔지니어링, 지원, 설계 및 유지 관리에 영향을 미치며 종종 데이터 레이크를 왜곡하여 중요한 시스템 데이터 흐름의 핵심 구성 요소가 됩니다. 이렇게 하는 것이 가능하지만 더 넓은 영향을 미칩니다. 이 접근 방식의 일반적인 요구 사항은 일괄 스트리밍으로 전환하여 다른 시스템에 제공되는 참조 데이터가 최대한 최신 상태가 되도록 하는 것입니다.

여러 경우에 스트리밍은 일단 실행되면 더 효과적이고 종종 더 비용 효율적인 대안이 될 수 있습니다. 스트리밍 시스템은 일반적으로 배치보다 구현이 간단하며 배치에 필요한 일반적인 수동 상태 전환 관리와는 달리 자동으로 상태를 처리할 수 있는 경우가 많습니다. 시간에 맞춰 정보를 전송하는 대신 스트리밍 시스템은 필요할 때 데이터를 전달할 수 있습니다. 오류가 발생하면 스트리밍 시스템은 전체 일괄 작업을 다시 시작하도록 요구하는 대신 단일 이벤트를 통과하거나 실패할 수 있습니다.

Apache Kafka, Samza, Apex, Spark Streaming, Storm 및 Apache Ni-Fi와 같은 기술이 주도하는 스트림 처리 혁신은 스트리밍 솔루션의 개발, 배포 및 유지 관리가 그 어느 때보다 쉬워졌다는 것을 의미합니다.   

> 많은 기업에서는 전통적으로 기존의 물리적 프로세스를 반영하는 일괄 처리를 선택해 왔습니다.
> 그러나 스트리밍은 더 저렴하고 간단한 접근 방식이 될 수 있을 정도로 성숙해졌습니다.
> 실행 및 유지 관리가 더 쉽고 데이터 레이크를 비즈니스의 중앙 참조 데이터 저장소로 사용하는 등 더 많은 사용 사례를 지원합니다.
> 이제 스트리밍이 시대에 맞는 접근 방식인 것 같습니다.  

#### Q4: 데이터에 라벨링을 어떤 방법으로 할 것인가?   
데이터 레이크는 더 광범위한 사용자 그룹이 다양한 비즈니스 사례에 대한 데이터를 수집하고 활용할 수 있도록 해주기 때문에 가치가 있을 수 있습니다. 그러나 본질적으로 그들은 종종 울창하고 야생의 숲입니다.
금융 서비스 산업은 복잡하며 원시 데이터세트에 저장된 정보는 오해의 소지가 있는 경우가 많습니다. 일부 회사는 데이터 세트를 분류하고 태그를 지정하여 보다 완전한 "데이터 사전"을 구축하는 기계 학습 기술과 같은 자동화된 접근 방식으로 대응했습니다. 다른 업체에서는 프로세스를 수동으로 수행하여 태그가 잘못 지정될 위험을 줄입니다. 일부는 데이터에 전혀 태그를 지정하지 않습니다. 그러나 가장 진보된 분류 기술로도 달성할 수 있는 것에는 한계가 있으며 잘못된 분류가 여전히 발생할 수 있다는 점을 미리 인식하는 것이 중요합니다. 여러 데이터 소스의 자동화된 태그 지정에 의존하는 것은 문제가 될 수 있으며 일반적으로
"교육받은 추측"으로 간주됩니다.
데이터를 탐색하는 데이터 과학자나 사용자의 전문 지식에 따라 데이터 사전의 정의가 매우 명확해야 할 수도 있습니다. 그럼에도 불구하고 데이터를 어떻게 적용하거나 연결해야 하는지에 대한 오해가 있을 수 있습니다.

또한 데이터 레이크는 신중하게 관리된 현재 데이터 소스라기보다는 저장소인 경우가 많다는 점을 기억하세요. 데이터 중복 및 품질 문제로 인해 데이터세트 분석이 중단되는 경우가 많습니다. 사용자는 잘못된 데이터를 오래된 데이터와 연결하거나 호환되지 않거나 변환되지 않은 정보를 결합하기 시작할 수 있습니다. 잘못된 데이터에 의존하면 흥미로운 컨셉이 신뢰할 수 있는 플랫폼이나 제품으로 도약하는 것을 막을 수 있습니다.

> 데이터 레이크에 대해 문제가 덜한 옵션을 고려하고 있다면 데이터를 보다 광범위하게 공개하기 위한 보다 점진적인 접근 방식으로 돌아가고 싶을 수도 있습니다.

신뢰할 수 없는 태그가 지정된 대량의 데이터를 열거나 방대한 양의 데이터에 직접 태그를 지정하는 대신 잠재력이 가장 큰 소규모 시스템 그룹으로 시작하여 진행에 따라 사전을 구축하세요.

데이터를 별개의 "영역"으로 정렬하고 그 안에 저장된 품질에 따라 분할할 수도 있습니다. 이는 방금 데이터 레이크로 전송된 원시 데이터를 위한 "랜딩 존"부터 깨끗하고 적절하게 구조화된 데이터를 포함하는 "정제된 존"까지 다양합니다.
일부 사용 사례에서는 이것이 강력한 접근 방식일 수 있지만 추가된 스토리지 및 구역화 요구 사항은 데이터를 원시 전용 또는 변환 전용으로 유지하는 것이 항상 바람직한 것은 아니라는 것을 의미합니다.
데이터에 대한 추가적인 감시가 흥미로운 혁신으로 이어질 수 있지만, 데이터 레이크가 단순히 신뢰할 수 없는 정보의 "늪"이 아닌지 확인하는 것이 중요합니다.  
> 라벨링은 유용한 데이터 레이크의 핵심이 될 수 있습니다.
> 기업은 모범 사례를 검토하고 개선하기 시작했으며, 데이터를 자동으로 분류하는 기계 학습 도구를 발전시켜 접근 방식을 향상할 수 있습니다.
> 그러나 우리는 주의할 점을 말씀드리고 싶습니다.
> 이러한 새로운 도구가 도움이 될 수 있지만 이것이 유일한 답은 아니며 일정량의 시행착오와 수동 태그 지정/분류도 적용되어야 합니다.

#### 데이터 접근제어는 어떻게 할 것인가? 
> DATA LAKE는 다양한 시스템의 데이터를 한 곳에서 볼 수 있는 기회를 제공합니다. 하지만 누가 무엇을 볼 수 있는지 어떻게 통제할 수 있나요?

데이터 레이크는 엄청나게 다양한 데이터의 본거지일 수 있지만 금융 서비스 회사가 이를 모든 사람에게 공개하는 것은 재앙이 될 수 있습니다. 기업은 다양한 장소에 데이터를 저장해 두고 있습니다. 경우에 따라 이 데이터의 소스는 시스템 자체 내에서 신중하게 정의된 규칙을 기반으로 데이터 액세스를 제한하는 경향이 있는 OLTP 시스템입니다. 이 데이터가 데이터 레이크로 전송되면 이러한 제한 사항이 사라지는 경우가 많습니다. 이는 정보에 대한 접근을 관리하기 위해 법적으로 요구되는 벽이 허물어진 것을 의미합니다. 부적절한 거래와 이해 상충을 방지하는 "중국 벽"을 위반하면 파괴적이고 비용이 많이 드는 결과를 초래할 수 있습니다.   

주요 규제 위반으로 인한 잠재적 비용을 고려하여 기업은 가능한 한 가장 빠른 시점에 액세스 제어를 고려하는 것이 좋습니다. 실제로 가장 좋은 접근 방식은 데이터 레이크 자체를 설계하는 동안 액세스 제어를 매핑하고 적용하는 것입니다. 그러나 금융 서비스 회사가 액세스 제어 시행을 나중으로 미루는 경우가 많습니다. 데이터 레이크가 개방형 API를 제공하도록 향상되거나 모든 형태의 클라이언트 액세스 기능을 제공한다면 이는 실제 문제가 될 수 있습니다.   

데이터 레이크의 잠재적 용량이 증가함에 따라 데이터 레이크를 처음 설계할 때 예상하지 못했던 긴급 액세스 제어 문제를 고려하는 것도 중요합니다. 예를 들어, 새로운 유형의 데이터가 데이터 레이크에 저장되는 경우 액세스 제어 방법을 개선해야 할 수 있습니다.   

> HADOOP용 APACHE RANGER와 같은 새로운 기술을 구성하여 기관의 데이터 액세스 제어 및 모니터링을 지원할 수 있습니다.

어떤 사용자가 어떤 데이터에 액세스할 수 있는지 정의하는 것 외에도 기업은 데이터에 적용하는 정책과 위반을 방지하기 위해 데이터를 암호화하고 마스킹하고 차단하는 방법을 고려하는 것이 중요합니다.   

이 모든 것이 표면적으로는 데이터 레이크를 사용하는 주요 동인 중 하나에 정면으로 맞서는 것처럼 보일 수 있습니다. 그러나 데이터를 한 곳에 모으는 경우 사용자가 볼 수 없어야 하는 데이터를 보지 못하게 하는 제어의 안전망도 제거됩니다. 귀하의 데이터에 접근할 수 있는 모든 데이터 과학자는 만리장성이 어디에 있는지, 만리장성의 한계는 어디인지 알고 있어야 합니다.   

> 접근 통제를 끝까지 맡기고 싶은 유혹이 너무 큽니다.
> 그러나 비즈니스에 중요한 애플리케이션에 데이터 레이크를 사용하기 시작하면서 이제 액세스 제어가 진정한 초점 영역이 되었습니다.
> 액세스 제어를 일찍 고려할수록 설계 수준(잠재적으로 별도의 영역을 가짐)과 기술 수준(신기술 사용) 모두에서 제어를 적용하는 것이 더 쉬워집니다.
> 기억하세요: 액세스 제어를 잘못하면 비용이 매우 높을 수 있습니다. 특히 데이터 유출이 발생하거나 연결이 허용되지 않는 데이터를 연결하는 경우에는 더욱 그렇습니다.
